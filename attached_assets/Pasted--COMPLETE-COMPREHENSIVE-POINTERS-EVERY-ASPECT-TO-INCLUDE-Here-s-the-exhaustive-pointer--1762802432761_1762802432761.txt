# üìã **COMPLETE COMPREHENSIVE POINTERS - EVERY ASPECT TO INCLUDE**

Here's the **exhaustive pointer list** covering **ALL aspects** from the specification document:

---

## üéØ **SECTION 1: CORE DATA ANALYSIS FEATURES**

### **1.1 DATA UPLOAD & IMPORT**

**üìç Include These:**
- ‚úÖ CSV upload with encoding detection (UTF-8, Latin-1, ISO-8859-1)
- ‚úÖ Excel files (.xlsx, .xls) with multi-sheet support
- ‚úÖ JSON files (flat and nested structures)
- ‚úÖ Parquet files (columnar format)
- ‚úÖ Drag-and-drop interface using `react-dropzone`
- ‚úÖ Progress bar showing upload percentage
- ‚úÖ File size validation (max 100MB recommended)
- ‚úÖ Preview data before confirmation
- ‚úÖ Batch upload (multiple files at once)
- ‚úÖ URL import (fetch data from web URLs)
- ‚úÖ Google Sheets integration
- ‚úÖ Database direct connection (PostgreSQL, MySQL, SQLite)
- ‚úÖ Cloud storage import (AWS S3, Google Cloud Storage, Azure Blob)

**üîß Technical Implementation:**
```python
# Backend: File parsing libraries
- pandas.read_csv() with encoding detection using chardet
- pandas.read_excel() with sheet_name parameter
- pandas.read_json() with normalize for nested structures
- pandas.read_parquet()
```

```typescript
// Frontend: Upload component
- react-dropzone for drag-and-drop
- axios onUploadProgress for progress tracking
- File type validation (.csv, .xlsx, .json, .parquet)
- Size limit enforcement
```

**üìÇ File Storage Structure:**
```
datasets/
  {user_id}/
    {session_id}/
      original_filename.csv
```

---

### **1.2 DATA QUALITY ASSESSMENT**

**üìç Include These:**
- ‚úÖ Overall quality score (0-100) with weighted formula:
  - **Completeness: 40%** (percentage of non-null values)
  - **Consistency: 30%** (format uniformity in text columns)
  - **Uniqueness: 20%** (duplicate row detection)
  - **Validity: 10%** (outlier detection in numeric columns)
- ‚úÖ Column-level quality metrics:
  - Missing value percentage per column
  - Unique value count
  - Data type identification
  - Sample values (first 5)
- ‚úÖ Automatic issue detection with severity levels:
  - **High severity**: >30% missing values
  - **Medium severity**: Duplicates, 10-30% missing
  - **Low severity**: Outliers, minor inconsistencies
- ‚úÖ Actionable recommendations:
  - "Impute missing values in 'age' using median"
  - "Remove duplicate rows to improve quality"
  - "Review outliers in 'price' column"
- ‚úÖ Quality trend tracking (before/after operations)
- ‚úÖ Quality report generation (PDF/HTML formats)
- ‚úÖ Data profiling dashboard

**üîß Technical Implementation:**
```python
# Backend: Quality calculation function
def calculate_quality_metrics(df: pd.DataFrame) -> dict:
    # 1. Completeness
    total_cells = len(df) * len(df.columns)
    missing_cells = df.isnull().sum().sum()
    completeness = 1 - (missing_cells / total_cells)
    
    # 2. Consistency (pattern matching)
    consistency = check_email_format() + check_phone_format()
    
    # 3. Uniqueness
    duplicates = df.duplicated().sum()
    uniqueness = 1 - (duplicates / len(df))
    
    # 4. Validity (outlier detection)
    validity = 1 - (outlier_count / total_values)
    
    # Weighted overall score
    overall = (completeness * 0.4 + consistency * 0.3 + 
               uniqueness * 0.2 + validity * 0.1) * 100
    
    return {
        'overall_score': overall,
        'completeness': completeness,
        'consistency': consistency,
        'uniqueness': uniqueness,
        'validity': validity,
        'column_metrics': {...},
        'issues': [...],
        'recommendations': [...]
    }
```

**üìä API Endpoint:**
```
GET /api/quality/{session_id}
Response: Quality metrics object
```

---

### **1.3 MISSING VALUE HANDLING**

**üìç Include These:**
- ‚úÖ **Detection**:
  - Count and percentage per column
  - Visual indicators in data preview (red highlighting)
  - Missing value heatmap visualization
- ‚úÖ **Imputation Methods**:
  - **Mean imputation** (for numeric columns with normal distribution)
  - **Median imputation** (for numeric columns with outliers)
  - **Mode imputation** (for categorical columns)
  - **KNN imputation** (K-Nearest Neighbors, configurable K=5)
  - **Forward fill** (propagate last valid value)
  - **Backward fill** (propagate next valid value)
  - **Interpolation** (linear, polynomial, spline for time series)
  - **Custom value fill** (user-specified value)
  - **MICE** (Multiple Imputation by Chained Equations)
  - **Model-based imputation** (regression prediction)
- ‚úÖ **Drop options**:
  - Drop rows with any missing values
  - Drop rows with missing values in specific columns
  - Drop columns with >X% missing values
- ‚úÖ **Results tracking**:
  - Missing values before/after
  - Number of values imputed
  - Method used
  - Affected rows/columns

**üîß Technical Implementation:**
```python
# Backend: Imputation function
def impute_missing(df: pd.DataFrame, columns: list, method: str) -> tuple:
    df_clean = df.copy()
    results = {}
    
    for col in columns:
        missing_before = df[col].isnull().sum()
        
        if method == 'mean':
            df_clean[col].fillna(df[col].mean(), inplace=True)
        elif method == 'median':
            df_clean[col].fillna(df[col].median(), inplace=True)
        elif method == 'mode':
            df_clean[col].fillna(df[col].mode()[0], inplace=True)
        elif method == 'knn':
            from sklearn.impute import KNNImputer
            imputer = KNNImputer(n_neighbors=5)
            df_clean[col] = imputer.fit_transform(df[[col]])
        elif method == 'forward_fill':
            df_clean[col].fillna(method='ffill', inplace=True)
        elif method == 'backward_fill':
            df_clean[col].fillna(method='bfill', inplace=True)
        
        missing_after = df_clean[col].isnull().sum()
        results[col] = {
            'missing_before': missing_before,
            'missing_after': missing_after,
            'imputed': missing_before - missing_after,
            'method': method
        }
    
    return df_clean, results
```

**üéØ AI Function Call:**
```python
# Groq/Gemini function definition
{
    "name": "impute_missing_values",
    "parameters": {
        "columns": ["age", "salary"],
        "method": "median"  # or "mean", "mode", "knn", "forward_fill", "backward_fill"
    }
}
```

---

### **1.4 OUTLIER DETECTION & REMOVAL**

**üìç Include These:**
- ‚úÖ **Detection Methods**:
  - **IQR (Interquartile Range)** with configurable multiplier (default 1.5)
    - Lower bound: Q1 - 1.5 √ó IQR
    - Upper bound: Q3 + 1.5 √ó IQR
  - **Z-Score** with configurable threshold (default 3)
    - Outlier if |z| > threshold
  - **Isolation Forest** (unsupervised ML, contamination=0.1)
  - **DBSCAN** (density-based clustering, eps & min_samples)
  - **Modified Z-Score** (using median instead of mean)
  - **Grubbs' test** (for normally distributed data)
  - **LOF** (Local Outlier Factor)
- ‚úÖ **Removal strategies**:
  - Remove outlier rows
  - Cap outliers (winsorization) at percentile limits
  - Replace with median/mean
  - Flag outliers without removal
- ‚úÖ **Visualization**:
  - Box plots showing outliers
  - Scatter plots with outliers highlighted
  - Distribution plots before/after removal
- ‚úÖ **Results tracking**:
  - Outliers detected per column
  - Total rows removed
  - Method used
  - Threshold values

**üîß Technical Implementation:**
```python
# Backend: Outlier detection function
def remove_outliers(df: pd.DataFrame, columns: list, method: str, threshold: float = 1.5):
    df_clean = df.copy()
    outliers_by_column = {}
    
    for col in columns:
        if method == 'iqr':
            Q1 = df_clean[col].quantile(0.25)
            Q3 = df_clean[col].quantile(0.75)
            IQR = Q3 - Q1
            lower = Q1 - threshold * IQR
            upper = Q3 + threshold * IQR
            mask = (df_clean[col] >= lower) & (df_clean[col] <= upper)
            
        elif method == 'zscore':
            z_scores = np.abs((df_clean[col] - df_clean[col].mean()) / df_clean[col].std())
            mask = z_scores < threshold
            
        elif method == 'isolation_forest':
            from sklearn.ensemble import IsolationForest
            iso = IsolationForest(contamination=0.1, random_state=42)
            predictions = iso.fit_predict(df_clean[[col]].dropna())
            mask = predictions != -1
            
        elif method == 'dbscan':
            from sklearn.cluster import DBSCAN
            dbscan = DBSCAN(eps=threshold, min_samples=5)
            labels = dbscan.fit_predict(df_clean[[col]].dropna())
            mask = labels != -1
        
        outliers = (~mask).sum()
        outliers_by_column[col] = outliers
        df_clean = df_clean[mask]
    
    return df_clean, {
        'method': method,
        'rows_before': len(df),
        'rows_after': len(df_clean),
        'total_removed': len(df) - len(df_clean),
        'outliers_by_column': outliers_by_column
    }
```

**üéØ AI Function Call:**
```python
{
    "name": "detect_and_remove_outliers",
    "parameters": {
        "columns": ["price", "quantity"],
        "method": "iqr",  # or "zscore", "isolation_forest", "dbscan"
        "threshold": 1.5
    }
}
```

---

### **1.5 DUPLICATE HANDLING**

**üìç Include These:**
- ‚úÖ **Detection types**:
  - Exact duplicates (all columns identical)
  - Partial duplicates (specific columns identical)
  - Fuzzy duplicates (similarity threshold using string matching)
- ‚úÖ **Removal strategies**:
  - Keep first occurrence
  - Keep last occurrence
  - Keep none (remove all duplicates)
  - Flag duplicates without removal
- ‚úÖ **Duplicate grouping**:
  - Group by specific columns
  - Show duplicate groups for review
- ‚úÖ **Results tracking**:
  - Number of duplicate rows found
  - Number of rows removed
  - Percentage of dataset affected
  - Duplicate group details

**üîß Technical Implementation:**
```python
# Backend: Duplicate removal function
def remove_duplicates(df: pd.DataFrame, subset=None, keep='first') -> tuple:
    rows_before = len(df)
    
    # Exact duplicates
    df_clean = df.drop_duplicates(subset=subset, keep=keep)
    
    rows_after = len(df_clean)
    duplicates_removed = rows_before - rows_after
    
    return df_clean, {
        'rows_removed': duplicates_removed,
        'rows_before': rows_before,
        'rows_after': rows_after,
        'percentage': (duplicates_removed / rows_before) * 100
    }

# Fuzzy duplicate detection
def find_fuzzy_duplicates(df: pd.DataFrame, column: str, threshold=0.85):
    from fuzzywuzzy import fuzz
    duplicates = []
    
    for i, val1 in enumerate(df[column]):
        for j, val2 in enumerate(df[column][i+1:], start=i+1):
            similarity = fuzz.ratio(str(val1), str(val2)) / 100
            if similarity >= threshold:
                duplicates.append((i, j, similarity))
    
    return duplicates
```

**üéØ AI Function Call:**
```python
{
    "name": "clean_dataset",
    "parameters": {
        "operations": ["remove_duplicates"],
        "keep": "first"  # or "last", "none"
    }
}
```

---

### **1.6 DATA CLEANING OPERATIONS**

**üìç Include These:**
- ‚úÖ **Text normalization**:
  - Convert to lowercase
  - Convert to uppercase
  - Title case formatting
- ‚úÖ **Whitespace handling**:
  - Remove leading spaces
  - Remove trailing spaces
  - Remove extra spaces (multiple spaces ‚Üí single space)
  - Strip all whitespace
- ‚úÖ **Special characters**:
  - Remove special characters
  - Replace specific characters
  - Remove punctuation
  - Unicode normalization
- ‚úÖ **Pattern extraction**:
  - Email validation and extraction (regex: `[\w\.-]+@[\w\.-]+\.\w+`)
  - Phone number formatting (various formats: +1-234-567-8900, (234) 567-8900)
  - URL validation and extraction
  - Extract numbers from text
  - Extract dates from text
- ‚úÖ **Date/time parsing**:
  - Auto-detect date formats
  - Standardize date formats (YYYY-MM-DD)
  - Extract date components (year, month, day, weekday)
  - Time zone conversion
- ‚úÖ **Currency handling**:
  - Remove currency symbols ($, ‚Ç¨, ¬£, ¬•)
  - Convert string to numeric
  - Currency conversion (USD to EUR, etc.)
- ‚úÖ **Unit conversion**:
  - Metric to imperial (kg to lbs, km to miles)
  - Imperial to metric
  - Temperature (Celsius ‚Üî Fahrenheit)
- ‚úÖ **String operations**:
  - Trim to max length
  - Pad with zeros/spaces
  - Remove HTML tags
  - Decode HTML entities

**üîß Technical Implementation:**
```python
# Backend: Text cleaning functions
def clean_text(df: pd.DataFrame, column: str, operations: list):
    df_clean = df.copy()
    
    for op in operations:
        if op == 'lowercase':
            df_clean[column] = df_clean[column].str.lower()
        elif op == 'uppercase':
            df_clean[column] = df_clean[column].str.upper()
        elif op == 'title_case':
            df_clean[column] = df_clean[column].str.title()
        elif op == 'remove_whitespace':
            df_clean[column] = df_clean[column].str.strip()
        elif op == 'remove_extra_spaces':
            df_clean[column] = df_clean[column].str.replace(r'\s+', ' ', regex=True)
        elif op == 'remove_special_chars':
            df_clean[column] = df_clean[column].str.replace(r'[^a-zA-Z0-9\s]', '', regex=True)
        elif op == 'extract_emails':
            df_clean[column + '_email'] = df_clean[column].str.extract(r'([\w\.-]+@[\w\.-]+\.\w+)')
        elif op == 'extract_phones':
            df_clean[column + '_phone'] = df_clean[column].str.extract(r'(\+?\d{1,3}[-.\s]?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4})')
        elif op == 'extract_urls':
            df_clean[column + '_url'] = df_clean[column].str.extract(r'(https?://[^\s]+)')
        elif op == 'remove_html':
            import re
            df_clean[column] = df_clean[column].apply(lambda x: re.sub(r'<[^>]+>', '', str(x)))
    
    return df_clean

# Currency cleaning
def clean_currency(df: pd.DataFrame, column: str):
    df_clean = df.copy()
    df_clean[column] = df_clean[column].str.replace(r'[$‚Ç¨¬£¬•,]', '', regex=True)
    df_clean[column] = pd.to_numeric(df_clean[column], errors='coerce')
    return df_clean
```

---

### **1.7 DATA TRANSFORMATION**

**üìç Include These:**
- ‚úÖ **Categorical Encoding**:
  - **Label Encoding**: Convert categories to integers (0, 1, 2, ...)
  - **One-Hot Encoding**: Create binary columns for each category
  - **Ordinal Encoding**: Map ordered categories (Small=1, Medium=2, Large=3)
  - **Target Encoding**: Encode based on target variable mean
  - **Binary Encoding**: Convert to binary representation
  - **Frequency Encoding**: Replace with category frequency
- ‚úÖ **Type Conversion**:
  - String to integer
  - String to float
  - String to datetime
  - Datetime to string
  - Boolean conversion
- ‚úÖ **Date Feature Extraction**:
  - Year, month, day
  - Day of week, day of year
  - Week of year
  - Quarter
  - Is weekend, is month end
  - Hour, minute, second (for timestamps)
- ‚úÖ **Mathematical Transformations**:
  - Log transformation (log, log10, log2)
  - Square root transformation
  - Power transformation (x¬≤, x¬≥)
  - Reciprocal (1/x)
  - Box-Cox transformation
- ‚úÖ **Binning/Discretization**:
  - Equal width binning
  - Equal frequency binning (quantile-based)
  - Custom bin edges
  - Age groups, salary ranges, etc.
- ‚úÖ **Scaling/Normalization**:
  - **Min-Max Scaling**: Scale to [0, 1] range
  - **Standard Scaling**: Z-score normalization (mean=0, std=1)
  - **Robust Scaling**: Using median and IQR (resistant to outliers)
  - **MaxAbs Scaling**: Scale by maximum absolute value

**üîß Technical Implementation:**
```python
# Backend: Encoding functions
def encode_categorical(df: pd.DataFrame, columns: list, method: str):
    df_clean = df.copy()
    results = {}
    
    for col in columns:
        if method == 'label':
            from sklearn.preprocessing import LabelEncoder
            le = LabelEncoder()
            df_clean[col] = le.fit_transform(df_clean[col].astype(str))
            results[col] = {
                'method': 'label',
                'unique_values': len(le.classes_),
                'mapping': dict(zip(le.classes_, le.transform(le.classes_)))
            }
            
        elif method == 'onehot':
            dummies = pd.get_dummies(df_clean[col], prefix=col)
            df_clean = pd.concat([df_clean, dummies], axis=1)
            df_clean.drop(col, axis=1, inplace=True)
            results[col] = {
                'method': 'onehot',
                'new_columns': dummies.columns.tolist()
            }
            
        elif method == 'ordinal':
            # Custom ordering
            categories = sorted(df_clean[col].unique())
            mapping = {cat: i for i, cat in enumerate(categories)}
            df_clean[col] = df_clean[col].map(mapping)
            results[col] = {
                'method': 'ordinal',
                'mapping': mapping
            }
    
    return df_clean, results

# Scaling functions
def scale_data(df: pd.DataFrame, columns: list, method: str):
    from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
    
    df_clean = df.copy()
    
    if method == 'minmax':
        scaler = MinMaxScaler()
    elif method == 'standard':
        scaler = StandardScaler()
    elif method == 'robust':
        scaler = RobustScaler()
    
    df_clean[columns] = scaler.fit_transform(df_clean[columns])
    
    return df_clean, {
        'method': method,
        'columns': columns,
        'parameters': scaler.get_params()
    }
```

**üéØ AI Function Call:**
```python
{
    "name": "encode_categorical",
    "parameters": {
        "columns": ["gender", "category"],
        "method": "onehot"  # or "label", "ordinal", "target"
    }
}
```

---

### **1.8 DATA FILTERING & SELECTION**

**üìç Include These:**
- ‚úÖ **Comparison operators**:
  - Greater than (>)
  - Less than (<)
  - Equal to (==)
  - Not equal to (!=)
  - Greater than or equal (>=)
  - Less than or equal (<=)
- ‚úÖ **Text operations**:
  - Contains substring
  - Starts with prefix
  - Ends with suffix
  - Regex pattern matching
  - Case-insensitive search
- ‚úÖ **Multiple conditions**:
  - AND logic (all conditions must be true)
  - OR logic (any condition must be true)
  - NOT logic (inverse condition)
  - Nested conditions
- ‚úÖ **Range filtering**:
  - Between two values
  - In list of values
  - Not in list
- ‚úÖ **Date range filtering**:
  - Between dates
  - Last N days/months/years
  - Specific date ranges
- ‚úÖ **Null filtering**:
  - Is null
  - Is not null
- ‚úÖ **Sampling**:
  - Top N rows
  - Bottom N rows
  - Random sampling (n rows or %)
  - Stratified sampling (maintain class distribution)

**üîß Technical Implementation:**
```python
# Backend: Filtering function
def filter_data(df: pd.DataFrame, conditions: list, logic='AND'):
    df_filtered = df.copy()
    
    masks = []
    for condition in conditions:
        column = condition['column']
        operator = condition['operator']
        value = condition['value']
        
        if operator == '>':
            mask = df_filtered[column] > value
        elif operator == '<':
            mask = df_filtered[column] < value
        elif operator == '==':
            mask = df_filtered[column] == value
        elif operator == '!=':
            mask = df_filtered[column] != value
        elif operator == '>=':
            mask = df_filtered[column] >= value
        elif operator == '<=':
            mask = df_filtered[column] <= value
        elif operator == 'contains':
            mask = df_filtered[column].str.contains(value, case=False, na=False)
        elif operator == 'startswith':
            mask = df_filtered[column].str.startswith(value, na=False)
        elif operator == 'endswith':
            mask = df_filtered[column].str.endswith(value, na=False)
        elif operator == 'in':
            mask = df_filtered[column].isin(value)
        elif operator == 'between':
            mask = df_filtered[column].between(value[0], value[1])
        elif operator == 'isnull':
            mask = df_filtered[column].isnull()
        elif operator == 'notnull':
            mask = df_filtered[column].notnull()
        
        masks.append(mask)
    
    # Combine masks
    if logic == 'AND':
        final_mask = pd.concat(masks, axis=1).all(axis=1)
    elif logic == 'OR':
        final_mask = pd.concat(masks, axis=1).any(axis=1)
    
    df_filtered = df_filtered[final_mask]
    
    return df_filtered, {
        'rows_before': len(df),
        'rows_after': len(df_filtered),
        'rows_filtered': len(df) - len(df_filtered)
    }
```

**üéØ AI Function Call:**
```python
{
    "name": "filter_data",
    "parameters": {
        "conditions": [
            {"column": "age", "operator": ">", "value": 25},
            {"column": "city", "operator": "==", "value": "New York"}
        ],
        "logic": "AND"
    }
}
```

---

### **1.9 STATISTICAL ANALYSIS**

**üìç Include These:**
- ‚úÖ **Descriptive Statistics**:
  - Mean (average)
  - Median (50th percentile)
  - Mode (most frequent value)
  - Standard deviation
  - Variance
  - Min, max values
  - Count (total non-null values)
  - Sum
- ‚úÖ **Percentiles & Quartiles**:
  - Q1 (25th percentile)
  - Q2 (50th percentile / median)
  - Q3 (75th percentile)
  - IQR (Interquartile Range)
  - Custom percentiles (1st, 5th, 10th, 90th, 95th, 99th)
- ‚úÖ **Distribution Analysis**:
  - Skewness (measure of asymmetry)
  - Kurtosis (measure of tailedness)
  - Histogram analysis
  - Probability distributions (normal, binomial, etc.)
- ‚úÖ **Correlation Analysis**:
  - **Pearson correlation** (linear relationships)
  - **Spearman correlation** (monotonic relationships)
  - **Kendall correlation** (rank-based)
  - Correlation matrix (all numeric columns)
  - Correlation heatmap visualization
- ‚úÖ **Covariance Matrix**:
  - Pairwise covariance
  - Covariance heatmap
- ‚úÖ **Hypothesis Testing**:
  - **T-test** (compare means of two groups)
  - **Chi-square test** (independence of categorical variables)
  - **ANOVA** (compare means of multiple groups)
  - **Kolmogorov-Smirnov test** (distribution comparison)
  - P-values and significance levels
- ‚úÖ **Confidence Intervals**:
  - 95% confidence interval for mean
  - 99% confidence interval
  - Custom confidence levels
- ‚úÖ **Cross-tabulation**:
  - Pivot tables
  - Frequency tables
  - Contingency tables
- ‚úÖ **Group-by Aggregations**:
  - Group by single column
  - Group by multiple columns
  - Aggregate functions (sum, mean, count, min, max)
- ‚úÖ **Rolling Statistics**:
  - Moving average (7-day, 30-day, etc.)
  - Rolling sum
  - Rolling standard deviation
  - Exponential moving average (EMA)

**üîß Technical Implementation:**
```python
# Backend: Statistical analysis function
def get_statistics(df: pd.DataFrame, columns: list = None, stats: list = None):
    if columns is None:
        columns = df.select_dtypes(include=[np.number]).columns.tolist()
    
    results = {}
    
    for col in columns:
        col_stats = {}
        
        if 'mean' in stats or stats is None:
            col_stats['mean'] = df[col].mean()
        if 'median' in stats or stats is None:
            col_stats['median'] = df[col].median()
        if 'mode' in stats or stats is None:
            col_stats['mode'] = df[col].mode()[0] if not df[col].mode().empty else None
        if 'std' in stats or stats is None:
            col_stats['std'] = df[col].std()
        if 'variance' in stats or stats is None:
            col_stats['variance'] = df[col].var()
        if 'min' in stats or stats is None:
            col_stats['min'] = df[col].min()
        if 'max' in stats or stats is None:
            col_stats['max'] = df[col].max()
        if 'count' in stats or stats is None:
            col_stats['count'] = df[col].count()
        if 'sum' in stats or stats is None:
            col_stats['sum'] = df[col].sum()
        if 'skewness' in stats:
            col_stats['skewness'] = df[col].skew()
        if 'kurtosis' in stats:
            col_stats['kurtosis'] = df[col].kurtosis()
        if 'percentiles' in stats:
            col_stats['q1'] = df[col].quantile(0.25)
            col_stats['q2'] = df[col].quantile(0.50)
            col_stats['q3'] = df[col].quantile(0.75)
            col_stats['iqr'] = col_stats['q3'] - col_stats['q1']
        
        results[col] = col_stats
    
    # Correlation matrix
    if 'correlation' in stats or stats is None:
        results['correlation_matrix'] = df[columns].corr(method='pearson').to_dict()
    
    return results

# Correlation calculation
def calculate_correlation(df: pd.DataFrame, method='pearson'):
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    if method == 'pearson':
        corr_matrix = df[numeric_cols].corr(method='pearson')
    elif method == 'spearman':
        corr_matrix = df[numeric_cols].corr(method='spearman')
    elif method == 'kendall':
        corr_matrix = df[numeric_cols].corr(method='kendall')
    
    return corr_matrix.to_dict()

# Hypothesis testing
def perform_ttest(df: pd.DataFrame, column1: str, column2: str):
    from scipy.stats import ttest_ind
    
    group1 = df[column1].dropna()
    group2 = df[column2].dropna()
    
    t_statistic, p_value = ttest_ind(group1, group2)
    
    return {
        't_statistic': t_statistic,
        'p_value': p_value,
        'significant': p_value < 0.05
    }
```

**üéØ AI Function Call:**
```python
{
    "name": "get_statistics",
    "parameters": {
        "columns": ["age", "salary", "price"],
        "stats": ["mean", "median", "std", "correlation"]
    }
}
```

---

## ü§ñ **SECTION 2: AI INTEGRATION (DUAL AI SYSTEM)**

### **2.1 GROQ API INTEGRATION**

**üìç Include These:**
- ‚úÖ **Model Selection**:
  - Primary: `llama3-70b-8192` (70B