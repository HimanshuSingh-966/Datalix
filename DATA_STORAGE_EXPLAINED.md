# DataLix AI - Data Storage Architecture

## Overview

DataLix AI uses a **hybrid storage architecture** that combines two different storage systems, each optimized for specific types of data:

1. **PostgreSQL Database** - For persistent user data and metadata
2. **In-Memory Python Sessions** - For active dataset analysis

## Why Two Storage Systems?

### The Problem
When analyzing datasets, you need:
- **Fast access** to perform calculations, transformations, and AI operations
- **Persistent storage** for user accounts, chat history, and session metadata
- **Temporary storage** that doesn't clutter your database with large datasets

### The Solution
We separate concerns by storing different types of data in different places:

---

## 1. PostgreSQL Database Storage

### What's Stored Here?
- **User accounts** (email, username, hashed passwords)
- **Session metadata** (session IDs, creation dates, user ownership)
- **Chat message history** (conversation text, timestamps, AI responses)
- **Embedded results** (chart configurations, data previews, function calls)

### Why PostgreSQL?
âœ… **Persistent** - Data survives server restarts  
âœ… **Searchable** - Query your past conversations  
âœ… **Secure** - Passwords are hashed with bcrypt  
âœ… **Relational** - Connect users â†’ sessions â†’ messages  
âœ… **Rollback support** - Integrated with Replit's database features  

### Database Schema
```
users
â”œâ”€ id (UUID)
â”œâ”€ email
â”œâ”€ username
â””â”€ password_hash

sessions
â”œâ”€ id (UUID)
â”œâ”€ user_id â†’ users.id
â”œâ”€ created_at
â”œâ”€ dataset_name
â””â”€ quality_score

messages
â”œâ”€ id (UUID)
â”œâ”€ session_id â†’ sessions.id
â”œâ”€ role (user/assistant)
â”œâ”€ content (text)
â”œâ”€ chart_data (JSONB)
â”œâ”€ data_preview (JSONB)
â””â”€ timestamp
```

### What This Means for You
- Your **account information** is always safe
- Your **conversation history** is saved and searchable
- You can **close the app** and come back later to see your past analyses
- **Multiple users** can use the system simultaneously without conflicts

---

## 2. In-Memory Python Session Storage

### What's Stored Here?
- **Full datasets** (pandas DataFrames with all your data)
- **Active transformations** (cleaned data, engineered features)
- **Calculation results** (statistics, correlations, ML models)
- **Temporary files** (for exports and processing)

### Why In-Memory?
âœ… **Lightning fast** - No database queries needed for calculations  
âœ… **Pandas native** - Direct access to all pandas/scikit-learn operations  
âœ… **No storage limits** - Database doesn't get bloated with large datasets  
âœ… **Automatic cleanup** - Old sessions are garbage collected  

### Data Structure
```python
DataProcessor.sessions = {
    "session_abc123": {
        "user_id": "user_xyz",
        "dataset": pandas.DataFrame(...),  # Your actual data
        "metadata": {
            "filename": "sales.csv",
            "rows": 10000,
            "columns": 15,
            "upload_time": datetime(...)
        },
        "quality_metrics": {...},
        "transformations": [...]
    }
}
```

### What This Means for You
- **Upload a file** â†’ It's loaded into Python's memory instantly
- **Run analysis** â†’ Calculations happen at pandas speed
- **Server restarts** â†’ Active datasets are lost (but metadata remains in database)
- **Large files** â†’ No database bloat, just Python memory usage

---

## Data Flow Example

Let's walk through what happens when you upload and analyze a dataset:

### Step 1: Upload
```
You: Upload "sales.csv" (500 MB)
â†“
Python Backend: 
  âœ“ Read file with pandas
  âœ“ Generate session_id: "abc123"
  âœ“ Store DataFrame in memory
  âœ“ Calculate quality score: 87/100
  âœ“ Save session metadata to PostgreSQL
    â†’ sessions table: {id: "abc123", user_id: "you", dataset_name: "sales.csv"}
  âœ“ Save initial AI message to PostgreSQL
    â†’ messages table: {content: "Dataset loaded...", data_preview: {...}}
```

### Step 2: Analysis
```
You: "Show correlation matrix"
â†“
Python Backend:
  âœ“ Lookup session "abc123" in memory (fast!)
  âœ“ Calculate correlation on DataFrame
  âœ“ Generate Plotly chart JSON
  âœ“ Save AI response to PostgreSQL
    â†’ messages table: {content: "Here's your correlation...", chart_data: {...}}
```

### Step 3: Transformation
```
You: "Remove outliers from price column"
â†“
Python Backend:
  âœ“ Modify DataFrame in memory
  âœ“ Update session.dataset with cleaned data
  âœ“ Calculate new quality score
  âœ“ Save transformation result to PostgreSQL
    â†’ messages table: {content: "Removed 23 outliers...", data_preview: {...}}
```

### Step 4: Export
```
You: "Export as CSV"
â†“
Python Backend:
  âœ“ Get DataFrame from memory session
  âœ“ Write to temporary file
  âœ“ Send file download to browser
  âœ“ (File gets deleted after download)
```

---

## Important Behaviors

### âœ… Persistent (Survives Restarts)
- Your login credentials
- All chat conversations
- Session history (names, dates, quality scores)
- Chart configurations and previews

### âŒ Temporary (Lost on Restart)
- The actual dataset (full DataFrame)
- In-progress transformations not yet saved
- Calculated statistics cached in memory
- ML models and clustering results

### ğŸ’¡ Best Practices

**For Long Analysis Sessions:**
1. Upload your dataset
2. Perform all your analysis and transformations
3. **Export the cleaned dataset** before closing
4. Your chat history will show what you did
5. Next time, re-upload the cleaned version

**For Quick Queries:**
1. Upload dataset
2. Ask questions, get insights
3. Download any charts you need
4. No need to save - conversation is already stored

**For Production Use:**
If you need datasets to persist across restarts, you can:
- Use Supabase Storage (integrated with Replit)
- Connect to external S3/GCS buckets
- Store datasets in PostgreSQL (for smaller files <10MB)

---

## Storage Limits

### PostgreSQL Database
- **Messages/Chat**: Virtually unlimited (text is compressed)
- **Chart data**: JSONB columns handle complex visualizations
- **Users/Sessions**: No practical limit

### In-Memory Python
- **Dataset size**: Limited by server RAM
- **Concurrent sessions**: Multiple users share memory pool
- **Session lifetime**: Active until server restart or explicit deletion

---

## Data Privacy & Security

### Database (PostgreSQL)
- âœ… Passwords hashed with bcrypt (never stored plain text)
- âœ… User data isolated by user_id foreign keys
- âœ… Sessions belong to specific users (no cross-user access)
- âœ… Managed by Replit's secure database service

### In-Memory (Python)
- âœ… Sessions isolated by session_id
- âœ… Authentication required for all operations
- âœ… Automatic cleanup of old sessions
- âš ï¸ No encryption at rest (data in RAM)
- âš ï¸ Lost on server restart

---

## Supabase Integration

When Supabase is configured (via environment variables), the authentication system uses:

- **Supabase Auth** for user management and JWT tokens
- **PostgreSQL** for session and message storage (as before)
- **In-Memory** for dataset storage (as before)

This gives you:
- Professional authentication with email verification
- Secure JWT token management
- Password reset capabilities
- OAuth integration options (Google, GitHub, etc.)

---

## Summary Table

| Data Type | Storage Location | Persistent? | Speed | Size Limit |
|-----------|-----------------|-------------|-------|------------|
| User accounts | PostgreSQL | âœ… Yes | Normal | Unlimited |
| Chat history | PostgreSQL | âœ… Yes | Normal | Unlimited |
| Session metadata | PostgreSQL | âœ… Yes | Normal | Unlimited |
| Full datasets | Python Memory | âŒ No | âš¡ Fast | RAM limit |
| Transformations | Python Memory | âŒ No | âš¡ Fast | RAM limit |
| Calculations | Python Memory | âŒ No | âš¡ Fast | RAM limit |

---

## Questions & Answers

**Q: What happens if the server restarts?**  
A: Your account, chat history, and session list remain intact. However, you'll need to re-upload datasets to continue analysis.

**Q: Can I access my old analyses?**  
A: Yes! Your chat history shows all past conversations, visualizations, and insights. You just need to re-upload the dataset to perform new operations.

**Q: Why not store everything in the database?**  
A: Databases are optimized for queries, not high-speed numerical calculations. Pandas operations on 1M rows in memory are 100x faster than database operations.

**Q: Is my data safe?**  
A: Yes. User authentication is secure (bcrypt hashing), and session access is protected by JWT tokens. Datasets in memory are isolated by session ID.

**Q: How long are datasets kept in memory?**  
A: Until the server restarts or the session is explicitly deleted. We recommend exporting important results.

---

## Future Enhancements

Planned improvements to the storage architecture:

1. **Redis caching** - For faster session lookups
2. **S3 integration** - Optional persistent dataset storage
3. **Automatic backups** - Periodic exports to cloud storage
4. **Session resurrection** - Auto-reload last dataset on login
5. **Collaborative sessions** - Share datasets with team members

---

*Last updated: 2025-11-11*
